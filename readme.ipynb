{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aleph Alpha Client\n",
    "\n",
    "[![PyPI version](https://badge.fury.io/py/aleph-alpha-client.svg)](https://pypi.org/project/aleph-alpha-client/)\n",
    "\n",
    "Interact with the Aleph Alpha API via Python\n",
    "\n",
    "> [Documentation of the HTTP API can be found here](https://docs.aleph-alpha.com/api/)\n",
    "\n",
    "## Installation\n",
    "\n",
    "The latest stable version is deployed to PyPi so you can install this package via pip.\n",
    "\n",
    "```sh\n",
    "pip install aleph-alpha-client\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Completion Multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 401] {'error': 'InvalidSignature', 'code': 'UNAUTHENTICATED'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=12'>13</a>\u001b[0m prompt \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=13'>14</a>\u001b[0m     image,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mQ: What does the picture show? A:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=15'>16</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=16'>17</a>\u001b[0m request \u001b[39m=\u001b[39m CompletionRequest(prompt\u001b[39m=\u001b[39mprompt, maximum_tokens\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=17'>18</a>\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcomplete(request)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/volker/workspace/aleph-open/aleph-alpha-client/readme.ipynb#ch0000001vscode-remote?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39mcompletions[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcompletion\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/aleph-open/aleph-alpha-client/aleph_alpha_client/aleph_alpha_model.py:13\u001b[0m, in \u001b[0;36mAlephAlphaModel.complete\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplete\u001b[39m(\u001b[39mself\u001b[39m, request: CompletionRequest) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponse:\n\u001b[0;32m---> 13\u001b[0m     response_json \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcomplete(model \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name, hosting\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhosting, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest\u001b[39m.\u001b[39;49m_asdict())\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m CompletionResponse\u001b[39m.\u001b[39mfrom_json(response_json)\n",
      "File \u001b[0;32m~/workspace/aleph-open/aleph-alpha-client/aleph_alpha_client/aleph_alpha_client.py:255\u001b[0m, in \u001b[0;36mAlephAlphaClient.complete\u001b[0;34m(self, model, prompt, hosting, maximum_tokens, temperature, top_k, top_p, presence_penalty, frequency_penalty, repetition_penalties_include_prompt, use_multiplicative_presence_penalty, best_of, n, logit_bias, log_probs, stop_sequences, tokens, disable_optimizations)\u001b[0m\n\u001b[1;32m    230\u001b[0m named_request \u001b[39m=\u001b[39m CompletionRequest(\n\u001b[1;32m    231\u001b[0m     prompt\u001b[39m=\u001b[39mprompt \u001b[39mor\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    232\u001b[0m     maximum_tokens\u001b[39m=\u001b[39mmaximum_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     disable_optimizations\u001b[39m=\u001b[39mdisable_optimizations,\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcomplete\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    251\u001b[0m     headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_headers,\n\u001b[1;32m    252\u001b[0m     json\u001b[39m=\u001b[39mnamed_request\u001b[39m.\u001b[39mrender_as_body(model, hosting),\n\u001b[1;32m    253\u001b[0m     timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m )\n\u001b[0;32m--> 255\u001b[0m response_json \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_translate_errors(response)\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m response_json\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moptimized_prompt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[39m# Return a message to the user that we optimized their prompt\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    259\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mWe optimized your prompt before sending it to the model. The optimized prompt is available at result[\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimized_prompt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]. If you do not want these optimizations applied, you can pass the disable_optimizations flag to your request.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/aleph-open/aleph-alpha-client/aleph_alpha_client/aleph_alpha_client.py:460\u001b[0m, in \u001b[0;36mAlephAlphaClient._translate_errors\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mjson())\n\u001b[1;32m    459\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mPermissionError\u001b[39;00m(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mjson())\n\u001b[1;32m    461\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m402\u001b[39m:\n\u001b[1;32m    462\u001b[0m     \u001b[39mraise\u001b[39;00m QuotaError(response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mjson())\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 401] {'error': 'InvalidSignature', 'code': 'UNAUTHENTICATED'}"
     ]
    }
   ],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaModel, AlephAlphaClient, CompletionRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "# You need to choose a model with multimodal capabilities for this example.\n",
    "url = \"https://cdn-images-1.medium.com/max/1200/1*HunNdlTmoPj8EKpl-jqvBA.png\"\n",
    "\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [\n",
    "    image,\n",
    "    \"Q: What does the picture show? A:\",\n",
    "]\n",
    "request = CompletionRequest(prompt=prompt, maximum_tokens=20)\n",
    "result = model.complete(request)\n",
    "\n",
    "print(result.completions[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, EvaluationRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "model = \"luminous-base\"\n",
    "request = EvaluationRequest(prompt=\"The api works\", completion_expected=\" well\")\n",
    "result = client.evaluate(model, request=request)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation Multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, EvaluationRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with multimodal capabilities for this example.\n",
    "model = \"luminous-base\"\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [\n",
    "    image,\n",
    "    \"Q: What is the name of the store?\\nA:\",\n",
    "]\n",
    "request = EvaluationRequest(prompt=prompt, completion_expected=\" Blockbuster Video\")\n",
    "result = client.evaluate(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding text prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, EmbeddingRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "model = \"luminous-base\"\n",
    "request = EmbeddingRequest(prompt=[\"This is an example.\"], layers=[-1], pooling=[\"mean\"])\n",
    "result = client.embed(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding multimodal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, EmbeddingRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with multimodal capabilities for this example.\n",
    "model = \"luminous-base\"\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [\n",
    "    image,\n",
    "    \"Q: What is the name of the store?\\nA:\",\n",
    "]\n",
    "request = EmbeddingRequest(prompt=prompt, layers=[-1], pooling=[\"mean\"])\n",
    "result = client.embed(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a Docx Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, AlephAlphaClient, QaRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support for this example.\n",
    "model = \"luminous-extended\"\n",
    "\n",
    "docx_file = \"./tests/sample.docx\"\n",
    "document = Document.from_docx_file(docx_file)\n",
    "\n",
    "request = QaRequest(\n",
    "    query = \"What is a computer program?\",\n",
    "    documents = [document]\n",
    ")\n",
    "\n",
    "result = client.qa(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, AlephAlphaClient, QaRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support for this example.\n",
    "model = \"luminous-extended\"\n",
    "\n",
    "prompt = \"In imperative programming, a computer program is a sequence of instructions in a programming language that a computer can execute or interpret.\"\n",
    "document = Document.from_text(prompt)\n",
    "\n",
    "request = QaRequest(\n",
    "    query = \"What is a computer program?\",\n",
    "    documents = [document],\n",
    ")\n",
    "\n",
    "result = client.qa(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a multimodal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, ImagePrompt, AlephAlphaClient, QaRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support and multimodal capabilities for this example.\n",
    "model = \"luminous-extended\"\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [image]\n",
    "document = Document.from_prompt(prompt)\n",
    "\n",
    "request = QaRequest (\n",
    "    query = \"What is the name of the store?\",\n",
    "    documents = [document]\n",
    ")\n",
    "\n",
    "result = client.qa(model, request=request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenize a text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, ImagePrompt, AlephAlphaClient, TokenizationRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support and multimodal capabilities for this example.\n",
    "model = \"luminous-extended\"\n",
    "request = TokenizationRequest(prompt=\"This is an example.\", tokens=True, token_ids=True)\n",
    "response = client.tokenize(model, request=request)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Detokenize a token IDs into text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, ImagePrompt, AlephAlphaClient, DetokenizationRequest\n",
    "import os\n",
    "\n",
    "client = AlephAlphaClient(\n",
    "    host=\"https://api.aleph-alpha.com\",\n",
    "    token=os.getenv(\"AA_TOKEN\")\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support and multimodal capabilities for this example.\n",
    "model = \"luminous-extended\"\n",
    "request = DetokenizationRequest(token_ids=[1730, 387, 300, 4377, 17])\n",
    "response = client.detokenize(model, request=request)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Testing\n",
    "\n",
    "Tests use pytests with (optional) coverage plugin. Install the locally cloned repo in editable mode with:\n",
    "\n",
    "```bash\n",
    "pip install -e .[test]\n",
    "```\n",
    "\n",
    "**Tests make api calls that reduce your quota!**\n",
    "\n",
    "### Run tests\n",
    "\n",
    "Tests can be run using pytest. Make sure to create a `.env` file with the following content:\n",
    "\n",
    "```env\n",
    "# test settings\n",
    "TEST_API_URL=https://test.api.aleph-alpha.com\n",
    "TEST_MODEL=luminous-base\n",
    "TEST_TOKEN=your_token\n",
    "```\n",
    "\n",
    "Instead of a token username and password can be used.\n",
    "\n",
    "```env\n",
    "# test settings\n",
    "TEST_API_URL=https://api.aleph-alpha.com\n",
    "TEST_MODEL=luminous-base\n",
    "TEST_USERNAME=your_username\n",
    "TEST_PASSWORD=your_password\n",
    "```\n",
    "\n",
    "* A coverage report can be created using the optional arguments --cov-report and --cov (see pytest documentation)\n",
    "* A subset of tests can be selected by pointing to the module within tests\n",
    "\n",
    "```bash\n",
    "# run all tests, output coverage report of aleph_alpha_client module in terminal\n",
    "pytest --cov-report term --cov=aleph_alpha_client tests\n",
    "pytest tests -v # start verbose\n",
    "```\n",
    "\n",
    "If an html coverage report has been created a simple http server can be run to serve static files.\n",
    "\n",
    "```bash\n",
    "python -m http.server --directory htmlcov 8000\n",
    "```\n",
    "\n",
    "## Update README\n",
    "\n",
    "> Do not change the README.md directly as it is generated from readme.ipynb \n",
    "\n",
    "To update the readme, do the following:\n",
    "\n",
    "1. `pip install -r requirements-dev.txt`\n",
    "\n",
    "2. Edit the notebook in your favorite jupyter editor and run all python cells to verify that the code examples still work.\n",
    "\n",
    "3. To generate a new README.md first remove all output cells from the Jupyter notebook and then execute the command: `jupyter nbconvert --to markdown readme.ipynb --output README.md`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8-aleph-alpha-client')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d222ed40ed0240cca423d298d0a37d8e7bd0dfb1af9103b378684293dbd36ad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
