{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aleph Alpha Client\n",
    "\n",
    "[![PyPI version](https://badge.fury.io/py/aleph-alpha-client.svg)](https://pypi.org/project/aleph-alpha-client/)\n",
    "\n",
    "Interact with the Aleph Alpha API via Python\n",
    "\n",
    "> [Documentation of the HTTP API can be found here](https://docs.aleph-alpha.com/api/)\n",
    "\n",
    "## Installation\n",
    "\n",
    "The latest stable version is deployed to PyPi so you can install this package via pip.\n",
    "\n",
    "```sh\n",
    "pip install aleph-alpha-client\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Completion Multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaModel, AlephAlphaClient, CompletionRequest, Prompt\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "# You need to choose a model with multimodal capabilities for this example.\n",
    "url = \"https://cdn-images-1.medium.com/max/1200/1*HunNdlTmoPj8EKpl-jqvBA.png\"\n",
    "\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = Prompt([\n",
    "    image,\n",
    "    \"Q: What does the picture show? A:\",\n",
    "])\n",
    "request = CompletionRequest(prompt=prompt, maximum_tokens=20)\n",
    "result = model.complete(request)\n",
    "\n",
    "print(result.completions[0].completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaClient, AlephAlphaModel, EvaluationRequest, Prompt\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "request = EvaluationRequest(prompt=Prompt.from_text(\"The api works\"), completion_expected=\" well\")\n",
    "result = model.evaluate(request)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation Multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, EvaluationRequest, Prompt\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with multimodal capabilities for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = Prompt([\n",
    "    image,\n",
    "    \"Q: What is the name of the store?\\nA:\",\n",
    "])\n",
    "request = EvaluationRequest(prompt=prompt, completion_expected=\" Blockbuster Video\")\n",
    "result = model.evaluate(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding text prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaModel, AlephAlphaClient, EmbeddingRequest, Prompt\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "request = EmbeddingRequest(prompt=Prompt.from_text(\"This is an example.\"), layers=[-1], pooling=[\"mean\"])\n",
    "result = model.embed(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding multimodal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, EmbeddingRequest, Prompt\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with multimodal capabilities for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = Prompt([\n",
    "    image,\n",
    "    \"Q: What is the name of the store?\\nA:\",\n",
    "])\n",
    "request = EmbeddingRequest(prompt=prompt, layers=[-1], pooling=[\"mean\"])\n",
    "result = model.embed(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, SemanticEmbeddingRequest, SemanticRepresentation, Prompt\n",
    "import math\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with multimodal capabilities for this example.\n",
    "    model_name = \"luminous-base\"\n",
    ")\n",
    "\n",
    "# Texts to compare\n",
    "texts = [\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"deep diving\",\n",
    "    \"artificial snow\",\n",
    "]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for text in texts:\n",
    "    request = SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Symmetric)\n",
    "    result = model.semantic_embed(request)\n",
    "    embeddings.append(result.embedding)\n",
    "\n",
    "# Calculate cosine similarities. Can use numpy or scipy or another library to do this\n",
    "def cosine_similarity(v1: Sequence[float], v2: Sequence[float]) -> float:\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_similarity(embeddings[0], embeddings[1])))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_similarity(embeddings[0], embeddings[2])))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[3], cosine_similarity(embeddings[0], embeddings[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, SemanticEmbeddingRequest, SemanticRepresentation, Prompt\n",
    "import math\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with multimodal capabilities for this example.\n",
    "    model_name = \"luminous-base\"\n",
    ")\n",
    "\n",
    "# Documents to search in\n",
    "documents = [\n",
    "    # AI wikipedia article\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\",\n",
    "    # Deep Learning Wikipedia article\n",
    "    \"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\",\n",
    "    # Deep Diving Wikipedia article\n",
    "    \"Deep diving is underwater diving to a depth beyond the norm accepted by the associated community. In some cases this is a prescribed limit established by an authority, while in others it is associated with a level of certification or training, and it may vary depending on whether the diving is recreational, technical or commercial. Nitrogen narcosis becomes a hazard below 30 metres (98 ft) and hypoxic breathing gas is required below 60 metres (200 ft) to lessen the risk of oxygen toxicity.\",\n",
    "]\n",
    "# Keyword to search documents with\n",
    "query = \"artificial intelligence\"\n",
    "\n",
    "# Embed Query\n",
    "request = SemanticEmbeddingRequest(prompt=Prompt.from_text(query), representation=SemanticRepresentation.Query)\n",
    "result = model.semantic_embed(request)\n",
    "query_embedding = result.embedding\n",
    "\n",
    "# Embed documents\n",
    "document_embeddings = []\n",
    "\n",
    "for document in documents:\n",
    "    request = SemanticEmbeddingRequest(prompt=Prompt.from_text(document), representation=SemanticRepresentation.Document)\n",
    "    result = model.semantic_embed(request)\n",
    "    document_embeddings.append(result.embedding)\n",
    "\n",
    "# Calculate cosine similarities. Can use numpy or scipy or another library to do this\n",
    "def cosine_similarity(v1: Sequence[float], v2: Sequence[float]) -> float:\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s...\\\" is: %.3f\" % (query, documents[0][:10], cosine_similarity(query_embedding, document_embeddings[0])))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s...\\\" is: %.3f\" % (query, documents[1][:10], cosine_similarity(query_embedding, document_embeddings[1])))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s...\\\" is: %.3f\" % (query, documents[2][:10], cosine_similarity(query_embedding, document_embeddings[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a Docx Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, AlephAlphaClient, AlephAlphaModel, QaRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "docx_file = \"./tests/sample.docx\"\n",
    "document = Document.from_docx_file(docx_file)\n",
    "\n",
    "request = QaRequest(\n",
    "    query = \"What is a computer program?\",\n",
    "    documents = [document]\n",
    ")\n",
    "\n",
    "result = model.qa(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaClient, AlephAlphaModel, QaRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "prompt = \"In imperative programming, a computer program is a sequence of instructions in a programming language that a computer can execute or interpret.\"\n",
    "document = Document.from_text(prompt)\n",
    "\n",
    "request = QaRequest(\n",
    "    query = \"What is a computer program?\",\n",
    "    documents = [document],\n",
    ")\n",
    "\n",
    "result = model.qa(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q&A with a multimodal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, ImagePrompt, AlephAlphaClient, AlephAlphaModel, QaRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [image]\n",
    "document = Document.from_prompt(prompt)\n",
    "\n",
    "request = QaRequest (\n",
    "    query = \"What is the name of the store?\",\n",
    "    documents = [document]\n",
    ")\n",
    "\n",
    "result = model.qa(request)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary with a Docx Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, AlephAlphaClient, AlephAlphaModel, SummarizationRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "docx_file = \"./tests/sample.docx\"\n",
    "document = Document.from_docx_file(docx_file)\n",
    "\n",
    "request = SummarizationRequest(document)\n",
    "\n",
    "result = model.summarize(request)\n",
    "\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary with a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaClient, AlephAlphaModel, SummarizationRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Imperative_programming\n",
    "prompt = \"\"\"In computer science, imperative programming is a programming paradigm of software that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates step by step, rather than on high-level descriptions of its expected results.\n",
    "\n",
    "The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying all the details of how the program should achieve the result.[1]\n",
    "\n",
    "Imperative and procedural programming\n",
    "\n",
    "Procedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavy procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.\n",
    "\n",
    "Procedural programming could be considered a step toward declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.\"\"\"\n",
    "document = Document.from_text(prompt)\n",
    "\n",
    "request = SummarizationRequest(document)\n",
    "\n",
    "result = model.summarize(request)\n",
    "\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary with a multimodal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Document, ImagePrompt, AlephAlphaClient, AlephAlphaModel, SummarizationRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    # You need to choose a model with qa support for this example.\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/2008-09-24_Blockbuster_in_Durham.jpg/330px-2008-09-24_Blockbuster_in_Durham.jpg\"\n",
    "image = ImagePrompt.from_url(url)\n",
    "prompt = [image]\n",
    "document = Document.from_prompt(prompt)\n",
    "\n",
    "request = SummarizationRequest(document)\n",
    "\n",
    "result = model.summarize(request)\n",
    "\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenize a text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaClient, AlephAlphaModel, TokenizationRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support and multimodal capabilities for this example.\n",
    "request = TokenizationRequest(prompt=\"This is an example.\", tokens=True, token_ids=True)\n",
    "response = model.tokenize(request)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Detokenize a token IDs into text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import AlephAlphaClient, AlephAlphaModel, DetokenizationRequest\n",
    "import os\n",
    "\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"AA_TOKEN\")),\n",
    "    model_name = \"luminous-extended\"\n",
    ")\n",
    "\n",
    "# You need to choose a model with qa support and multimodal capabilities for this example.\n",
    "request = DetokenizationRequest(token_ids=[1730, 387, 300, 4377, 17])\n",
    "response = model.detokenize(request)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Testing\n",
    "\n",
    "Tests use pytests with (optional) coverage plugin. Install the locally cloned repo in editable mode with:\n",
    "\n",
    "```bash\n",
    "pip install -e .[test]\n",
    "```\n",
    "\n",
    "**Tests make api calls that reduce your quota!**\n",
    "\n",
    "### Run tests\n",
    "\n",
    "Tests can be run using pytest. Make sure to create a `.env` file with the following content:\n",
    "\n",
    "```env\n",
    "# test settings\n",
    "TEST_API_URL=https://test.api.aleph-alpha.com\n",
    "TEST_MODEL=luminous-base\n",
    "TEST_TOKEN=your_token\n",
    "```\n",
    "\n",
    "Instead of a token username and password can be used.\n",
    "\n",
    "```env\n",
    "# test settings\n",
    "TEST_API_URL=https://api.aleph-alpha.com\n",
    "TEST_MODEL=luminous-base\n",
    "TEST_USERNAME=your_username\n",
    "TEST_PASSWORD=your_password\n",
    "```\n",
    "\n",
    "* A coverage report can be created using the optional arguments --cov-report and --cov (see pytest documentation)\n",
    "* A subset of tests can be selected by pointing to the module within tests\n",
    "\n",
    "```bash\n",
    "# run all tests, output coverage report of aleph_alpha_client module in terminal\n",
    "pytest --cov-report term --cov=aleph_alpha_client tests\n",
    "pytest tests -v # start verbose\n",
    "```\n",
    "\n",
    "If an html coverage report has been created a simple http server can be run to serve static files.\n",
    "\n",
    "```bash\n",
    "python -m http.server --directory htmlcov 8000\n",
    "```\n",
    "\n",
    "## Update README\n",
    "\n",
    "> Do not change the README.md directly as it is generated from readme.ipynb \n",
    "\n",
    "To update the readme, do the following:\n",
    "\n",
    "1. `pip install -e .[dev]`\n",
    "\n",
    "2. Edit the notebook in your favorite jupyter editor and run all python cells to verify that the code examples still work.\n",
    "\n",
    "3. To generate a new README.md first remove all output cells from the Jupyter notebook and then execute the command: `jupyter nbconvert --to markdown readme.ipynb --output README.md`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1cf196a61db1dcae1cf7f7eff5a2e71137eb6892f7e0624acb7f9d46d623fef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
